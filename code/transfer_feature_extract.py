# -*- coding: utf-8 -*-
"""transfer_feat.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13RxivOAnI553yLmaCpKptSNqqKNu5UvO
"""

!pip install tensorflow-gpu

"""# First imports and checks, gpu used to increase the computation speed"""

import tensorflow as tf
import numpy as np

print(tf.__version__)

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""# Import dataset"""

import os
from google.colab import drive

drive.mount('/content/drive')

dataset = '/content/drive/My Drive/dataset_3200_224'
testset = '/content/drive/My Drive/test_800_224'
to_save = '/content/drive/My Drive/models'

"""# Dataset split"""

import keras
from keras.preprocessing.image import ImageDataGenerator

#Generate batches of tensor image data with real-time data augmentation, here used to rescale and split the dataset
#duplicates are removed by default
datagen = ImageDataGenerator(
    rescale = 1. / 255,
    rotation_range=30,
    horizontal_flip=True
)

#Takes the path to a directory & generates batches of augmented data.
#When classes not provided, the list of classes is automatically inferred from the subdirectory names/structure
#default batch = 32
train_generator = datagen.flow_from_directory(
    directory = dataset,
    target_size = (224, 224)
)


test_datagen = ImageDataGenerator(
    rescale = 1. / 255,
)

test_generator = test_datagen.flow_from_directory(
    directory = testset,
    target_size = (224, 224)
)

classnames = [k for k,v in train_generator.class_indices.items()]
print("Image input %s" %str( train_generator.image_shape))
print("Classes: %r \n" %classnames)

print('Loaded %d training samples from %d classes.' %( train_generator.n, train_generator.num_classes))
print('Loaded %d test samples from %d classes.' %( test_generator.n, test_generator.num_classes))

"""# Show some examples"""

import matplotlib.pyplot as plt

x,y = train_generator.next()

for i in range(0,10):
    image = x[i]
    label = y[i].argmax() 
    print(classnames[label])
    plt.imshow(image)
    plt.show()

"""# Create the transfer-model"""

#feature extraction
pretrained_model = tf.keras.applications.vgg16.VGG16(input_shape=train_generator.image_shape,
                                               include_top=False,
                                               weights='imagenet')
pretrained_model.trainable = False

model = tf.keras.Sequential([
    pretrained_model,
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')
])

model.summary()

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

import time

#Total number of steps (batches of samples) to yield from generator before declaring one epoch finished
steps = train_generator.n//train_generator.batch_size

start = time.time()
try:
    history = model.fit_generator(train_generator, epochs=100, verbose=1,
                    steps_per_epoch = steps,
                    validation_data = test_generator,
                    validation_steps = test_generator.n//test_generator.batch_size,
                    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=8, restore_best_weights=True)]
                    )
except KeyboardInterrupt:
    pass
end = time.time()
enc_time = end-start

print('Execution time:')
print(str(enc_time))
print()

"""# Save the model"""

filename = os.path.join(to_save, 'transf2.h5')
model.save(filename)
print("\nModel saved successfully on file %s\n" %filename)

"""# Evaluate the model

Accuracy on test set
"""

#needed to put shuffle = false
test_generator = test_datagen.flow_from_directory(
    directory = testset,
    target_size = (224, 224),
    shuffle=False
)

val_steps = test_generator.n//test_generator.batch_size
loss, acc = model.evaluate_generator( test_generator, verbose=1, steps = val_steps)
print('Test loss: %f' %loss)
print('Test accuracy: %f' %acc)

"""Precision, recall, F-score"""

import sklearn.metrics 
from sklearn.metrics import classification_report, confusion_matrix

print('Loaded %d test samples from %d classes.' %(test_generator.n, test_generator.num_classes))

preds = model.predict_generator(test_generator, verbose=1, steps=val_steps)

Ypred = np.argmax(preds, axis=1)
Ytest = test_generator.classes  # shuffle=False in test_generator

print(classification_report(Ytest, Ypred, labels=None, target_names=classnames, digits=3))

"""Graphs"""

import matplotlib.pyplot as plt

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""confusion matrix"""

cm = confusion_matrix(Ytest, Ypred)
print(cm)

conf = [] # data structure for confusions: list of (i,j,cm[i][j])
for i in range(0,cm.shape[0]):
  for j in range(0,cm.shape[1]):
    if (i!=j and cm[i][j]>0):
      conf.append([i,j,cm[i][j]])

col=2
conf = np.array(conf)
conf = conf[np.argsort(-conf[:,col])]  # decreasing order by 3-rd column (i.e., cm[i][j])

print('\nConfusion matrix analysis' )
print('%-16s     %-16s  \t%s \t%s ' %('True','Predicted','errors','err %'))
print('------------------------------------------------------------------')
for k in conf:
  print('%-16s ->  %-16s  \t%d \t%.2f %% ' %(classnames[k[0]],classnames[k[1]],k[2],k[2]*100.0/test_generator.n))

"""# Smart-I test"""

smartI = '/content/drive/My Drive/test_Weather_Dataset'
print('TEST ON SMART I')


test_generator = test_datagen.flow_from_directory(
    directory = smartI,
    target_size = (224, 224),
    shuffle=False
)


val_steps = test_generator.n//test_generator.batch_size
loss, acc = model.evaluate_generator( test_generator, verbose=1, steps = val_steps)
print('Test loss: %f' %loss)
print('Test accuracy: %f' %acc)

import sklearn.metrics 
from sklearn.metrics import classification_report, confusion_matrix

smartI = '/content/drive/My Drive/test_Weather_Dataset'
print('TEST ON SMART I')


test_generator = test_datagen.flow_from_directory(
    directory = smartI,
    target_size = (224, 224),
    shuffle=False
)

preds = model.predict_generator(test_generator, verbose=1, steps=val_steps)
val_steps = test_generator.n//test_generator.batch_size+1

Ypred = np.argmax(preds, axis=1)
Ytest = test_generator.classes  # shuffle=False in test_generator

print(classification_report(Ytest, Ypred, labels=None, target_names=classnames, digits=3))



cm = confusion_matrix(Ytest, Ypred)
print(cm)

conf = [] # data structure for confusions: list of (i,j,cm[i][j])
for i in range(0,cm.shape[0]):
  for j in range(0,cm.shape[1]):
    if (i!=j and cm[i][j]>0):
      conf.append([i,j,cm[i][j]])

col=2
conf = np.array(conf)
conf = conf[np.argsort(-conf[:,col])]  # decreasing order by 3-rd column (i.e., cm[i][j])

print('\nConfusion matrix analysis' )
print('%-16s     %-16s  \t%s \t%s ' %('True','Predicted','errors','err %'))
print('------------------------------------------------------------------')
for k in conf:
  print('%-16s ->  %-16s  \t%d \t%.2f %% ' %(classnames[k[0]],classnames[k[1]],k[2],k[2]*100.0/test_generator.n))

"""# blind test"""

files = '/content/drive/My Drive/WeatherBlindTestSet'

test_generator = test_datagen.flow_from_directory(
    directory = files,
    target_size = (224, 224),
    shuffle=False
)


val_steps = test_generator.n//test_generator.batch_size+1
preds = model.predict_generator(test_generator, verbose=1, steps=val_steps)

print(preds)

Ypred = np.argmax(preds, axis=1)
np.set_printoptions(threshold=np.inf)


res = []
sun = 0
haz = 0 
ran = 0 
sn = 0;
for i in Ypred:
  if i == 0:
    res.append('HAZE')
    haz += 1
  if i == 1:
    res.append('RAINY')
    ran += 1
  if i == 2:
    res.append('SNOWY')
    sn += 1
  if i == 3:
    res.append('SUNNY')
    sun += 1

print(res)
print(Ypred)

print(len(res))
print(len(Ypred))
print(sun)
print(haz)
print(ran)
print(sn)




results = pd.DataFrame(res, columns=['predictions'])
from google.colab import files
results.to_csv('prediction.csv') 
files.download('prediction.csv')