# -*- coding: utf-8 -*-
"""AlexNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y8MBGFsFVWZjnuTM7oCfhnwjhSCEnqLR
"""

!pip install tensorflow-gpu

"""# First imports and checks, gpu used to increase the computation speed"""

import tensorflow as tf
import numpy as np

print(tf.__version__)

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""# Import dataset"""

import os
from google.colab import drive

drive.mount('/content/drive')

dataset = '/content/drive/My Drive/dataset_3200_224'
testset = '/content/drive/My Drive/test_800_224'
to_save = '/content/drive/My Drive/models'

"""# Dataset split"""

import keras
from keras.preprocessing.image import ImageDataGenerator

#Generate batches of tensor image data with real-time data augmentation, here used to rescale and split the dataset
#duplicates are removed by default
datagen = ImageDataGenerator(
    rescale = 1. / 255,
    rotation_range=90,
    width_shift_range=[-10,10],
    height_shift_range=[-10,10],
    horizontal_flip=True,
    vertical_flip=True
)

#Takes the path to a directory & generates batches of augmented data.
#When classes not provided, the list of classes is automatically inferred from the subdirectory names/structure
#default batch = 32
train_generator = datagen.flow_from_directory(
    directory = dataset,
    target_size = (224, 224)
)


test_datagen = ImageDataGenerator(
    rescale = 1. / 255,
)

test_generator = test_datagen.flow_from_directory(
    directory = testset,
    target_size = (224, 224)
)

classnames = [k for k,v in train_generator.class_indices.items()]
print("Image input %s" %str( train_generator.image_shape))
print("Classes: %r \n" %classnames)

print('Loaded %d training samples from %d classes.' %( train_generator.n, train_generator.num_classes))
print('Loaded %d test samples from %d classes.' %( test_generator.n, test_generator.num_classes))

"""# Show some examples"""

import matplotlib.pyplot as plt

x,y = train_generator.next()

for i in range(0,10):
    image = x[i]
    label = y[i].argmax() 
    print(classnames[label])
    plt.imshow(image)
    plt.show()

"""# Create the first model"""

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Activation, Dropout, Flatten,\
                         Conv2D, MaxPooling2D
from tensorflow.python.keras.layers.normalization import BatchNormalization
from tensorflow.python.keras import regularizers


def AlexNet(input_shape, num_classes, regl2 = 0.0001, lr=0.0001):

    model = Sequential()

    # C1 Convolutional Layer 
    model.add(Conv2D(filters=96, input_shape=input_shape, kernel_size=(11,11),\
                     strides=(2,2), padding='valid'))
    model.add(Activation('relu'))
    # Pooling
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
    # Batch Normalisation before passing it to the next layer
    model.add(BatchNormalization())

    # C2 Convolutional Layer
    model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))
    model.add(Activation('relu'))
    # Pooling
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
    # Batch Normalisation
    model.add(BatchNormalization())

    # C3 Convolutional Layer
    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))
    model.add(Activation('relu'))
    # Batch Normalisation
    model.add(BatchNormalization())

    # C4 Convolutional Layer
    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))
    model.add(Activation('relu'))
    # Batch Normalisation
    model.add(BatchNormalization())

    # C5 Convolutional Layer
    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))
    model.add(Activation('relu'))
    # Pooling
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
    # Batch Normalisation
    model.add(BatchNormalization())

    # Flatten
    model.add(Flatten())

    flatten_shape = (input_shape[0]*input_shape[1]*input_shape[2],)
    
    # D1 Dense Layer
    model.add(Dense(4096, input_shape=flatten_shape, kernel_regularizer=regularizers.l2(regl2)))
    model.add(Activation('relu'))
    # Dropout
    model.add(Dropout(0.4))
    # Batch Normalisation
    model.add(BatchNormalization())

    # D2 Dense Layer
    model.add(Dense(4096, kernel_regularizer=regularizers.l2(regl2)))
    model.add(Activation('relu'))
    # Dropout
    model.add(Dropout(0.4))
    # Batch Normalisation
    model.add(BatchNormalization())

    # D3 Dense Layer
    model.add(Dense(1000,kernel_regularizer=regularizers.l2(regl2)))
    model.add(Activation('relu'))
    # Dropout
    model.add(Dropout(0.4))
    # Batch Normalisation
    model.add(BatchNormalization())

    # Output Layer
    model.add(Dense(num_classes))
    model.add(Activation('softmax'))

    # Compile

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model
 
# create the model
model = AlexNet(train_generator.image_shape, train_generator.num_classes)
model.summary()

"""# Train the model"""

import time

#Total number of steps (batches of samples) to yield from generator before declaring one epoch finished
steps = train_generator.n//train_generator.batch_size

start = time.time()
try:
    history = model.fit_generator(train_generator, epochs=20, verbose=1,
                    steps_per_epoch = steps,
                    validation_data = test_generator,
                    validation_steps = test_generator.n//test_generator.batch_size
                    )
except KeyboardInterrupt:
    pass
end = time.time()
enc_time = end-start

print('Execution time:')
print(str(enc_time))
print()

"""# Save the model"""

filename = os.path.join(to_save, 'alex.h5')
model.save(filename)
print("\nModel saved successfully on file %s\n" %filename)

"""# Evaluate the model

Accuracy on test set
"""

#needed to put shuffle = false
test_generator = test_datagen.flow_from_directory(
    directory = testset,
    target_size = (224, 224),
    shuffle=False
)

val_steps = test_generator.n//test_generator.batch_size
loss, acc = model.evaluate_generator( test_generator, verbose=1, steps = val_steps)
print('Test loss: %f' %loss)
print('Test accuracy: %f' %acc)

"""Precision, recall, F-score"""

import sklearn.metrics 
from sklearn.metrics import classification_report, confusion_matrix

print('Loaded %d test samples from %d classes.' %(test_generator.n, test_generator.num_classes))

preds = model.predict_generator(test_generator, verbose=1, steps=val_steps)

Ypred = np.argmax(preds, axis=1)
Ytest = test_generator.classes  # shuffle=False in test_generator

print(classification_report(Ytest, Ypred, labels=None, target_names=classnames, digits=3))

"""Graphs"""

import matplotlib.pyplot as plt

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""confusion matrix"""

cm = confusion_matrix(Ytest, Ypred)
print(cm)

conf = [] # data structure for confusions: list of (i,j,cm[i][j])
for i in range(0,cm.shape[0]):
  for j in range(0,cm.shape[1]):
    if (i!=j and cm[i][j]>0):
      conf.append([i,j,cm[i][j]])

col=2
conf = np.array(conf)
conf = conf[np.argsort(-conf[:,col])]  # decreasing order by 3-rd column (i.e., cm[i][j])

print('\nConfusion matrix analysis' )
print('%-16s     %-16s  \t%s \t%s ' %('True','Predicted','errors','err %'))
print('------------------------------------------------------------------')
for k in conf:
  print('%-16s ->  %-16s  \t%d \t%.2f %% ' %(classnames[k[0]],classnames[k[1]],k[2],k[2]*100.0/test_generator.n))